{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Extract Bug Reports With Stack Traces - Defects4j 2.0\n",
    "This script is the first script to be executed. It extract all the basic necessary information for the defects4j bugs with stack traces.\n",
    "The Chart bugs were skipped due to the fact that their repo is not git based"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Vars declaration\n",
    "Update it according to the computer in which I am running the code and the current requirements\n",
    "\n",
    "Required paths:\n",
    "- bug_reports_path: It is the path to the folder that contains the textual information from the bug reports from defects4j. This files were extracted by An Ran, and are in this project's repo. They are either a json file or a txt file, depending on the project.\n",
    "- defects4j_path: It is the path in which  the defects4j 2.0 repo is cloned (https://github.com/rjust/defects4j)\n",
    "- output_file_path: It is the path to the file in which the json outputed by this script will be stored"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import os\n",
    "from secrets import base_path\n",
    "import importlib\n",
    "\n",
    "\n",
    "paths_dict=  {\n",
    "        \"bug_reports_textual_info_path\": os.path.join(base_path, \"DeepDiveBugReportsWithLogs\", \"data\", \"defects4j-2.0-bug-reports-textual-info\", \"an-ran-files\"),\n",
    "        \"defects4j_path\": os.path.join(base_path, \"defects4j\"),\n",
    "        \"output_file_path\": os.path.join(base_path, \"DeepDiveBugReportsWithLogs\", \"data\", \"bug_reports_with_stack_traces_details.json\"),\n",
    "        \"bug_info_csv_path\": os.path.join(base_path, \"DeepDiveBugReportsWithLogs\", \"data\", \"nakhla_bugs_data.csv\")\n",
    "}\n",
    "\n",
    "stack_trace_regex = r'(?m)^.*?Exception.*(?:\\n+^\\s*at .*)+'\n",
    "\n",
    "defects4j_projects_folder = {\n",
    "    # \"Chart\": \"jfreechart\",\n",
    "    \"Cli\": \"commons-cli.git\",\n",
    "    \"Closure\": \"closure-compiler.git\",\n",
    "    \"Codec\": \"commons-codec.git\",\n",
    "    \"Collections\": \"commons-collections.git\",\n",
    "    \"Compress\": \"commons-compress.git\",\n",
    "    \"Csv\": \"commons-csv.git\",\n",
    "    \"Gson\": \"gson.git\",\n",
    "    \"JacksonCore\": \"jackson-core.git\",\n",
    "    \"JacksonDatabind\": \"jackson-databind.git\",\n",
    "    \"Jsoup\": \"jsoup.git\",\n",
    "    \"JxPath\": \"commons-jxpath.git\",\n",
    "    \"Lang\": \"commons-lang.git\",\n",
    "    \"Math\": \"commons-math.git\",\n",
    "    \"Mockito\": \"mockito.git\",\n",
    "    \"Time\" : \"joda-time.git\"\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T15:14:12.841113500Z",
     "start_time": "2023-07-10T15:14:12.836113700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Getting the bug reports with log snippets or stack traces\n",
    "This block of code utilizes regex to look for log snippets and stack traces in the bugs textual information"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91 bug reports with logs found\n",
      "Collected info added to the file C:\\Users\\loren\\Concordia\\Masters\\DeepDiveBugReportsWithLogs\\data\\bug_reports_with_stack_traces_details.json\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "\n",
    "def find_stack_traces_in_txt_files (bug_id, text_content):\n",
    "    utils.find_regex_and_add_results_to_dict(stack_trace_regex, text_content, regex_result, bug_id)\n",
    "\n",
    "def find_stack_traces_in_json_files (bug_id, bug_report_json):\n",
    "    string_fields_list = [\"summary\", \"description\"]\n",
    "    for field in string_fields_list:\n",
    "        if field in bug_report_json.keys():\n",
    "            utils.find_regex_and_add_results_to_dict(stack_trace_regex, bug_report_json[field], regex_result, bug_id)\n",
    "    # Iterating through all the comments\n",
    "    if \"comments\" in bug_report_json.keys():\n",
    "        for comment in bug_report_json[\"comments\"]:\n",
    "            utils.find_regex_and_add_results_to_dict(stack_trace_regex, comment[\"content\"], regex_result, bug_id)\n",
    "\n",
    "# txt files\n",
    "bugs_data = {}\n",
    "regex_result = {}\n",
    "for file in glob.glob(os.path.join(paths_dict[\"bug_reports_textual_info_path\"], \"*.txt\")):\n",
    "    bug_id = os.path.basename(file).replace('.txt', '')\n",
    "    if bug_id.startswith(\"Chart\"):\n",
    "        continue\n",
    "    with open(file, 'r', encoding='utf-8') as file_obj:\n",
    "        file_content = file_obj.read()\n",
    "    find_stack_traces_in_txt_files(bug_id, file_content)\n",
    "\n",
    "\n",
    "# json files\n",
    "for file in glob.glob(os.path.join(paths_dict[\"bug_reports_textual_info_path\"] , \"*.json\")):\n",
    "    bug_id = os.path.basename(file).replace('.json', '')\n",
    "    if bug_id.startswith(\"Chart\"):\n",
    "        continue\n",
    "    bug_report_json = utils.json_file_to_dict(file)\n",
    "    find_stack_traces_in_json_files(bug_id, bug_report_json)\n",
    "\n",
    "number_of_bug_reports_with_logs = len(regex_result.keys())\n",
    "print(str(number_of_bug_reports_with_logs) + \" bug reports with logs found\")\n",
    "\n",
    "#for bug_report_file in regex_result.keys():\n",
    "for bug_report in regex_result.keys():\n",
    "    #bug_report = bug_report_file.split(\".\")[0] #Removing the file extension\n",
    "    project = bug_report.split(\"_\")[0]\n",
    "    bug_id = bug_report.split(\"_\")[1]\n",
    "    if project not in bugs_data.keys():\n",
    "        bugs_data[project] = {}\n",
    "    bugs_data[project][bug_id] = {\"stack_traces\": regex_result[bug_report]}\n",
    "\n",
    "utils.dict_to_json_file(paths_dict[\"output_file_path\"],bugs_data)\n",
    "print(\"Collected info added to the file \" + paths_dict[\"output_file_path\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T15:21:01.994857100Z",
     "start_time": "2023-07-10T15:21:01.873157200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Getting the defects4j information for each of these bugs\n",
    "Extracts the hash of the buggy commit, the bugfix commit and also the bug report ID.\n",
    "Obs: it requires that defects4j is cloned locally"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected info added to the file C:\\Users\\loren\\Concordia\\Masters\\DeepDiveBugReportsWithLogs\\data\\bug_reports_with_stack_traces_details.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "bugs_data = utils.json_file_to_dict(paths_dict[\"output_file_path\"])\n",
    "for project in bugs_data:\n",
    "    file_path = os.path.join(paths_dict[\"defects4j_path\"], \"framework\", \"projects\", project, \"active-bugs.csv\")\n",
    "    with open(file_path, 'r') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        for row in csv_reader:\n",
    "            if row[0] in bugs_data[project].keys():\n",
    "                bug_id = row[0]\n",
    "                bugs_data[project][bug_id][\"buggy_commit\"] = row[1]\n",
    "                bugs_data[project][bug_id][\"bugfix_commit\"] = row[2]\n",
    "                bugs_data[project][bug_id][\"bug_report_id\"] = row[3]\n",
    "                bugs_data[project][bug_id][\"bug_report_url\"] = row[4]\n",
    "\n",
    "utils.dict_to_json_file(paths_dict[\"output_file_path\"],bugs_data)\n",
    "print(\"Collected info added to the file \" + paths_dict[\"output_file_path\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T15:23:06.407553Z",
     "start_time": "2023-07-10T15:23:06.348083Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Collecting information about the defects4j trigger tests for each bug"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected info added to the file C:\\Users\\loren\\Concordia\\Masters\\DeepDiveBugReportsWithLogs\\data\\bug_reports_with_stack_traces_details.json\n"
     ]
    }
   ],
   "source": [
    "bugs_data = utils.json_file_to_dict(paths_dict[\"output_file_path\"])\n",
    "for project in bugs_data:\n",
    "    for bug_id in bugs_data[project]:\n",
    "        bugs_data[project][bug_id][\"defects4j_trigger_tests\"] = []\n",
    "        path = os.path.join(paths_dict[\"defects4j_path\"], \"framework\", \"projects\", project, \"trigger_tests\")\n",
    "        lines = utils.read_file_lines(bug_id, path)\n",
    "        for line in lines:\n",
    "            if line.startswith(\"--- \"):\n",
    "                failing_test_name = line.replace(\"--- \", \"\").replace(\"\\n\", \"\")\n",
    "                bugs_data[project][bug_id][\"defects4j_trigger_tests\"].append(failing_test_name)\n",
    "\n",
    "utils.dict_to_json_file(paths_dict[\"output_file_path\"], bugs_data)\n",
    "print(\"Collected info added to the file \" + paths_dict[\"output_file_path\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T15:23:08.650710100Z",
     "start_time": "2023-07-10T15:23:08.259598800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Getting the bug report commit hash for each of these bugs\n",
    "Extracts the report creation date, based on the tool found in the bug report url.\n",
    "Obs: In the case of sourceforge.net, there is no api available. Since they are only 2 bugs, the creation date was manually collected and added to the manually_extracted_dates dictionary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected info added to the file C:\\Users\\loren\\Concordia\\Masters\\DeepDiveBugReportsWithLogs\\data\\bug_reports_with_stack_traces_details.json\n"
     ]
    }
   ],
   "source": [
    "# There is not api for https://sourceforge.net and https://code.google.com/archive\n",
    "manually_extracted_dates = {\n",
    "    \"Chart\": {\n",
    "        \"5\": \"2008-05-01T00:00:00Z\"\n",
    "    },\n",
    "    \"Time\": {\n",
    "        \"14\": \"2012-05-22T00:00:00Z\"\n",
    "    },\n",
    "    \"Mockito\": {\n",
    "        \"17\": \"2009-11-20T00:00:00Z\" ,\n",
    "        \"22\": \"2014-04-06T00:00:00Z\",\n",
    "        \"25\": \"2010-11-11T00:00:00Z\",\n",
    "        \"30\": \"2010-10-15T00:00:00Z\",\n",
    "        \"31\": \"2010-10-15T00:00:00Z\",\n",
    "        \"35\": \"2009-06-19T00:00:00Z\"\n",
    "    }\n",
    "}\n",
    "\n",
    "bugs_data = utils.json_file_to_dict(paths_dict[\"output_file_path\"])\n",
    "\n",
    "for project in bugs_data:\n",
    "    for bug_id in bugs_data[project]:\n",
    "        bug_report_url = bugs_data[project][bug_id][\"bug_report_url\"]\n",
    "        if \"github\" in bug_report_url:\n",
    "            bugs_data[project][bug_id][\"bug_report_creation_date\"] = utils.extract_creation_date_from_github_issues(bug_report_url)\n",
    "        elif \"jira\" in bug_report_url:\n",
    "            bugs_data[project][bug_id][\"bug_report_creation_date\"] = utils.extract_creation_date_from_jira(bug_report_url)\n",
    "        elif \"storage.googleapis.com\" in bug_report_url:\n",
    "            bugs_data[project][bug_id][\"bug_report_creation_date\"] = utils.extract_creation_date_from_google_code(bug_report_url)\n",
    "        else:\n",
    "            try:\n",
    "                bugs_data[project][bug_id][\"bug_report_creation_date\"] = manually_extracted_dates[project][bug_id]\n",
    "            except KeyError:\n",
    "                bugs_data[project][bug_id][\"bug_report_creation_date\"] = None\n",
    "        if not bugs_data[project][bug_id][\"bug_report_creation_date\"]:\n",
    "            print(\"No bug_report_creation_date found for bug \" + project + \"_\" + bug_id)\n",
    "\n",
    "utils.dict_to_json_file(paths_dict[\"output_file_path\"], bugs_data)\n",
    "print(\"Collected info added to the file \" + paths_dict[\"output_file_path\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T15:23:35.533065Z",
     "start_time": "2023-07-10T15:23:10.175106600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Getting the bug report commit hash\n",
    "Bug report commit hash is defined as of the first commit before the bug report creation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected info added to the file C:\\Users\\loren\\Concordia\\Masters\\DeepDiveBugReportsWithLogs\\data\\bug_reports_with_stack_traces_details.json\n"
     ]
    }
   ],
   "source": [
    "bugs_data = utils.json_file_to_dict(paths_dict[\"output_file_path\"])\n",
    "\n",
    "master_branches_dict = {\n",
    "    \"default\": \"master\",\n",
    "    \"Lang\": \"trunk\",\n",
    "    \"Math\": \"trunk\"\n",
    "}\n",
    "for project in bugs_data:\n",
    "    master_branch = master_branches_dict.get(project, master_branches_dict['default'])\n",
    "    project_repo_path = os.path.join(paths_dict[\"defects4j_path\"], \"project_repos\", defects4j_projects_folder[project])\n",
    "    for bug_id in bugs_data[project]:\n",
    "        bug_report_creation_date = bugs_data[project][bug_id][\"bug_report_creation_date\"]\n",
    "        bug_report_commit_hash = utils.get_first_commit_before_date(project_repo_path, bug_report_creation_date, master_branch)\n",
    "        bugs_data[project][bug_id][\"bug_report_commit_hash\"] = bug_report_commit_hash\n",
    "\n",
    "utils.dict_to_json_file(paths_dict[\"output_file_path\"], bugs_data)\n",
    "print(\"Collected info added to the file \" + paths_dict[\"output_file_path\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T15:24:06.326065300Z",
     "start_time": "2023-07-10T15:23:39.278450900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
